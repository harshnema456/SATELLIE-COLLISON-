# ======================================================
# ğŸš€ PHASE 8: Data Processing & Feature Extraction
# Author: Harsh Nema
# Project: Advanced Satellite Trajectory Risk Avoidance
# ======================================================

import pandas as pd
import spiceypy as spice
import os
import numpy as np
from tqdm import tqdm
import urllib.request

# ======================================================
#  PATH CONFIGURATION (Auto-detect project root)
# ======================================================
# ğŸ‘‡ Dynamically detect the main project folder, so the script works no matter
#    where you execute it from (root or scripts/).
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(BASE_DIR, "data")
OUTPUT_FILE = os.path.join(DATA_DIR, "processed_dataset.csv")


# ======================================================
# 0ï¸âƒ£ AUTO-DOWNLOAD + LOAD LEAP SECONDS KERNEL
# ======================================================
def load_leapseconds_kernel():
    """Ensure NAIF leap seconds kernel (.tls) exists and load it."""
    kernel_path = os.path.join(DATA_DIR, "naif0012.tls")

    # ğŸ”¹ If file doesnâ€™t exist, download from NAIF server
    if not os.path.exists(kernel_path):
        print("ğŸŒ Downloading NAIF leap seconds kernel (naif0012.tls)...")
        urllib.request.urlretrieve(
            "https://naif.jpl.nasa.gov/pub/naif/generic_kernels/lsk/naif0012.tls",
            kernel_path
        )
        print("âœ… Downloaded and saved â†’ data/naif0012.tls")

    # ğŸ”¹ Load the kernel so SPICE can convert times correctly
    spice.furnsh(kernel_path)
    print("ğŸ§­ Loaded leap seconds kernel successfully!")


# ======================================================
# 1ï¸âƒ£ LOAD OBSERVER DATA (CSV)
# ======================================================
def load_observer_data():
    """Load the observer_data.csv file generated by nasa_fetcher.py."""
    path = os.path.join(DATA_DIR, "observer_data.csv")
    if not os.path.exists(path):
        # âš ï¸ Throw clear error if file is missing
        raise FileNotFoundError("âš ï¸ observer_data.csv not found. Run nasa_fetcher.py first.")
    
    df = pd.read_csv(path)
    print(f"ğŸ“„ Loaded observer data: {len(df)} records")
    return df


# ======================================================
# 2ï¸âƒ£ LOAD SPK TRAJECTORY FILE
# ======================================================
def load_spk_file():
    """Locate and load the latest .bsp trajectory file from /data."""
    spk_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".bsp")]
    if not spk_files:
        raise FileNotFoundError("âš ï¸ No .bsp file found. Run nasa_fetcher.py first.")
    
    spk_path = os.path.join(DATA_DIR, spk_files[0])
    spice.furnsh(spk_path)
    print(f"ğŸ›°ï¸ Loaded SPK file: {spk_path}")
    return spk_path


# ======================================================
# 3ï¸âƒ£ EXTRACT POSITION & VELOCITY VECTORS
# ======================================================
def extract_vectors(spk_path, center="399", frame="J2000", step=3600):
    """
    Extract state vectors (x, y, z, vx, vy, vz) hourly from SPK file.
    Automatically detects target ID and valid time coverage window.
    """
    print("ğŸ“¡ Extracting position and velocity vectors from SPK file...")

    # ğŸ”¹ Ensure leap seconds kernel is loaded
    load_leapseconds_kernel()

    # ğŸ”¹ Auto-detect target body ID from SPK
    ids = spice.spkobj(spk_path)
    target_id = list(ids)[0]
    target = str(target_id)
    print(f"ğŸ›°ï¸ Using detected target ID from SPK: {target}")

    # ğŸ”¹ Get coverage window for that target
    cover = spice.spkcov(spk_path, target_id)

    # ğŸ§  Handle both 1D and 2D cover formats
    try:
        # Some kernels return nested lists (e.g., [[start, stop]])
        et_start = float(cover[0][0])
        et_stop = float(cover[-1][1])
    except TypeError:
        # Others return a flat 1D array (e.g., [start, stop])
        et_start = float(cover[0])
        et_stop = float(cover[1])

    print(f"ğŸ•’ SPK Coverage: {spice.et2utc(et_start, 'C', 0)} â†’ {spice.et2utc(et_stop, 'C', 0)}")

    # ğŸ”¹ Create 1-hour time steps
    times = np.arange(et_start, et_stop, step)
    results = []

    # ğŸ”¹ Loop through time steps and extract position & velocity
    for t in tqdm(times, desc="Processing SPK Data"):
        try:
            state, lt = spice.spkezr(target, t, frame, "NONE", center)
            x, y, z, vx, vy, vz = state
            utc_time = spice.et2utc(t, "C", 0)
            results.append([utc_time, x, y, z, vx, vy, vz])
        except Exception as e:
            if "SPKINSUFFDATA" in str(e):
                continue  # skip out-of-range timestamps silently
            else:
                print(f"âš ï¸ Skipping time step due to error: {e}")

    # ğŸ”¹ Convert results into DataFrame
    df = pd.DataFrame(results, columns=["UTC_Time", "x", "y", "z", "vx", "vy", "vz"])
    print(f"âœ… Extracted {len(df)} vector records.")
    return df


# ======================================================
# 4ï¸âƒ£ MERGE OBSERVER + SPK DATA
# ======================================================
def merge_data(spk_df, obs_df):
    """Merge observer (ephemeris) and SPK (trajectory) datasets."""
    print("ğŸ”„ Merging observer and SPK datasets...")

    # ğŸ”¹ Convert time columns to datetime for merging
    if "Date_UT" in obs_df.columns:
        obs_df["Date_UT"] = pd.to_datetime(obs_df["Date_UT"], errors="coerce")
    if "UTC_Time" in spk_df.columns:
        spk_df["UTC_Time"] = pd.to_datetime(spk_df["UTC_Time"], errors="coerce")

    # ğŸ”¹ Merge nearest times (within Â±1 hour)
    merged = pd.merge_asof(
        obs_df.sort_values("Date_UT"),
        spk_df.sort_values("UTC_Time"),
        left_on="Date_UT",
        right_on="UTC_Time",
        direction="nearest",
        tolerance=pd.Timedelta("1H")
    )

    print(f"âœ… Merged dataset: {len(merged)} records.")
    return merged


# ======================================================
# 5ï¸âƒ£ MAIN EXECUTION
# ======================================================
def main():
    print("ğŸš€ Starting Phase 8: Data Processing & Feature Extraction...\n")

    # Step 1 â†’ Load observer CSV data
    obs_df = load_observer_data()

    # Step 2 â†’ Load SPK binary trajectory file
    spk_path = load_spk_file()

    # Step 3 â†’ Extract vector data from SPK
    spk_df = extract_vectors(spk_path)

    # Step 4 â†’ Merge observer + SPK data
    merged_df = merge_data(spk_df, obs_df)

    # Step 5 â†’ Save processed dataset for ML model
    merged_df.to_csv(OUTPUT_FILE, index=False)
    print(f"\nğŸ’¾ Final processed dataset saved â†’ {OUTPUT_FILE}")
    print("\nâœ… Phase 8 completed successfully â€” Data ready for model training.")


# ======================================================
# 6ï¸âƒ£ RUN SCRIPT
# ======================================================
if __name__ == "__main__":
    main()
